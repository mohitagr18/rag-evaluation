{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Metrics Deep Dive\n",
    "\n",
    "**Understanding How RAG Evaluation Metrics Work**\n",
    "\n",
    "\n",
    "## The 6 Metrics We'll Explore\n",
    "\n",
    "| # | Metric | Evaluates | Key Question |\n",
    "|---|--------|-----------|-------------|\n",
    "| 1 | **Faithfulness** | Generator | Is the answer grounded in context? |\n",
    "| 2 | **Answer Relevancy** | Generator | Does the answer address the question? |\n",
    "| 3 | **Context Precision** | Retriever | Are relevant chunks ranked at the top? |\n",
    "| 4 | **Context Recall** | Retriever | Did we retrieve all necessary information? |\n",
    "| 5 | **Context Entity Recall** | Retriever | Did we retrieve all important entities? |\n",
    "| 6 | **Noise Sensitivity** | System | Does noise cause wrong answers? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Setup & Environment\n",
    "\n",
    "Let's set up our environment with all required imports and configure our LLM/Embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found - please set it in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Imports\n",
    "\n",
    "# Standard library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and Embeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Async helper ready\n"
     ]
    }
   ],
   "source": [
    "# Helper function for running async code in Jupyter\n",
    "\n",
    "def run_async(coro):\n",
    "    \"\"\"Helper to run async code in Jupyter notebooks\"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # We're in Jupyter with an existing loop\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "print(\"âœ… Async helper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Faithfulness\n",
    "\n",
    "**Faithfulness** checks if the generated answer *sticks to the facts* from the retrieved context. It detects **hallucinations** - when the LLM makes things up that aren't in the source material.\n",
    "\n",
    "\n",
    "### ðŸ”§ How It Works (3 Steps)\n",
    "\n",
    "```\n",
    "Step 1: Extract Claims --> Step 2: Verify each claim against Context --> Step 3: Calculate Faithfulness\n",
    "```\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Faithfulness} = \\frac{\\text{Number of claims supported by context}}{\\text{Total number of claims}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Claims \n",
    "\n",
    "First, we use an LLM to break the answer into atomic, verifiable statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims:\n",
      "1. You are allowed $200 per person.\n",
      "2. Alcohol is pre-approved.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup the data\n",
    "bot_answer = \"You are allowed $200 per person, and alcohol is pre-approved.\"\n",
    "retrieved_context = \"Employees may spend up to $75 per person on client dinners. Receipts are required\"\n",
    "\n",
    "# 2. Define the extraction prompt \n",
    "claim_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following response, extract ALL factual claims as a numbered list.\n",
    "Each claim should be a single, verifiable statement.\n",
    "\n",
    "Response: {response}\n",
    "\"\"\")\n",
    "\n",
    "# 3. Run Extraction\n",
    "claims_chain = claim_prompt | llm | StrOutputParser()\n",
    "\n",
    "claims = claims_chain.invoke({\"response\": bot_answer})\n",
    "print(\"Claims:\")\n",
    "print(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Verify Against Context \n",
    "\n",
    "Next, we check each atomic claim against the source text to detect hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: 'You are allowed $200 per person.' -> NOT SUPPORTED (Score: 0)\n",
      "Claim: 'Alcohol is pre-approved.' -> NOT SUPPORTED (Score: 0)\n"
     ]
    }
   ],
   "source": [
    "# Define the Verification Prompt\n",
    "verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following context and claim, determine if the claim is SUPPORTED by the context.\n",
    "Answer \"SUPPORTED\" or \"NOT SUPPORTED\".\n",
    "\n",
    "Context: {context}\n",
    "Claim: {claim}\n",
    "\"\"\")\n",
    "\n",
    "verify_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "# List of extracted claims (from Step 1)\n",
    "claim_list = [\n",
    "    \"You are allowed $200 per person.\",\n",
    "    \"Alcohol is pre-approved.\"\n",
    "]\n",
    "\n",
    "# Run Verification\n",
    "results = []\n",
    "for claim in claim_list:\n",
    "    verdict = verify_chain.invoke({\n",
    "        \"context\": retrieved_context, \n",
    "        \"claim\": claim\n",
    "    })\n",
    "\n",
    "    if \"NOT SUPPORTED\" in verdict:\n",
    "        score = 0\n",
    "    elif \"SUPPORTED\" in verdict:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0 \n",
    "\n",
    "    results.append(score)\n",
    "    print(f\"Claim: '{claim}' -> {verdict} (Score: {score})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Manual faithfulness calculation\n",
    "\n",
    "faithfulness_score = sum(results) / len(results)\n",
    "print(f\"Faithfulness Score: {faithfulness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Answer Relevancy Deep Dive\n",
    "\n",
    "### What Answer Relevancy Measures\n",
    "\n",
    "**Answer Relevancy** checks if the answer *actually answers* the question asked. It doesn't care if the answer is factually correct - just whether it's relevant to the question.\n",
    "\n",
    "\n",
    "### ðŸ”§ The \"Reverse Engineering\" Approach\n",
    "\n",
    "Instead of directly comparing the answer to the question, we can:\n",
    "\n",
    "1. **Generates hypothetical questions** from the answer (\"What questions would this be a good answer to?\")\n",
    "2. **Compares embeddings** of generated questions with the original question\n",
    "3. **Calculates similarity** - if the generated questions are similar to the original, the answer is relevant!\n",
    "\n",
    "```\n",
    "Generate Questions --> Embed All Questions --> Calculate Similarity   \n",
    "```\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{cosine\\_similarity}(Hypothetical_{Question_i}, Original_{Question})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reverse Engineering (Hypothetical Question Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What is the client dinner allowance?\n",
      "Generated: \n",
      "1. When is the deadline for submitting travel expenses?  \n",
      "2. How should I submit my travel expenses for reimbursement?  \n",
      "3. What platform do I need to use to report my travel expenses?  \n"
     ]
    }
   ],
   "source": [
    "# 1. Setup data\n",
    "original_question = \"What is the client dinner allowance?\"\n",
    "bot_answer = \"Travel expenses must be submitted via Workday by Friday.\"\n",
    "\n",
    "# 2. Define the Reverse Engineering Prompt\n",
    "reverse_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following answer, generate 3 distinct questions that this answer would be a good response to.\n",
    "Answer: {answer}\n",
    "Generate 3 Questions (one per line):\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\"\"\")\n",
    "\n",
    "# 3. Run Generation\n",
    "gen_chain = reverse_prompt | llm | StrOutputParser()\n",
    "generated_output = gen_chain.invoke({\"answer\": bot_answer})\n",
    "\n",
    "print(f\"Original: {original_question}\")\n",
    "print(f\"Generated: \\n{generated_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_questions = [\n",
    "    \"When is the deadline for submitting travel expenses?\",\n",
    "    \"How should I submit my travel expenses for reimbursement?\",\n",
    "    \"What platform do I need to use to report my travel expenses?\"\n",
    "]\n",
    "\n",
    "# Function to calculate Cosine Similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# 1. Get Embeddings for the Original Question\n",
    "vec_original = embeddings.embed_query(original_question)\n",
    "\n",
    "# 2. Loop through Hypothetical Questions and calculate similarity\n",
    "scores = []\n",
    "for q in hypothetical_questions:\n",
    "    vec_hypo = embeddings.embed_query(q)\n",
    "    similarity = cosine_similarity(vec_original, vec_hypo)\n",
    "    scores.append(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Similarities: [0.2912332475805235, 0.28654404773454767, 0.23218916334013615]\n",
      "Final Answer Relevancy Score: 0.2700\n"
     ]
    }
   ],
   "source": [
    "# 3. Average the scores\n",
    "relevancy_score = sum(scores) / len(scores)\n",
    "\n",
    "print(f\"Individual Similarities: {scores}\")\n",
    "print(f\"Final Answer Relevancy Score: {relevancy_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Context Precision Deep Dive\n",
    "\n",
    "### What Context Precision Measures\n",
    "\n",
    "**Context Precision** evaluates whether the *most relevant chunks are ranked at the top* of your retrieval results. It's about **ranking quality**, not just whether you retrieved relevant information.\n",
    "\n",
    "\n",
    "### ðŸ”§ How It Works\n",
    "\n",
    "1. For each retrieved chunk, determine if it's **relevant** to the question/reference\n",
    "2. Calculate **Precision@K** at each position (weighted by position)\n",
    "3. Relevant chunks at the **top** = higher score\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Context Precision@K} = \\frac{\\sum_{k=1}^{K} (\\text{Precision@k} \\times \\text{relevance}_k)}{\\text{Total relevant items in top K}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Relevance Verification (The Judge) \n",
    "We ask the LLM to judge each chunk individually against the Ground Truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying Chunks...\n",
      "Chunk: 'Employees may spend up to $75 ...' -> RELEVANT (1)\n",
      "Chunk: 'All travel expenses must be re...' -> NOT RELEVANT (1)\n",
      "Chunk: 'The breakfast allowance is $20...' -> NOT RELEVANT (1)\n",
      "Chunk: 'The company HQ is located in N...' -> NOT RELEVANT (1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup data\n",
    "question = \"What is the dinner allowance?\"\n",
    "ground_truth = \"Employees may spend up to $75 on client dinners.\"\n",
    "retrieved_chunks = [\n",
    "    \"Employees may spend up to $75 on client dinners.\",\n",
    "    \"All travel expenses must be reported in Workday.\",\n",
    "    \"The breakfast allowance is $20 per day.\",\n",
    "    \"The company HQ is located in New York.\"\n",
    "]\n",
    "\n",
    "# 2. Define the Verification Prompt\n",
    "# We ask the LLM to act as a binary judge\n",
    "relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the Question and Ground Truth, determine if the following Context Chunk is RELEVANT.\n",
    "\n",
    "Question: {question}\n",
    "Ground Truth: {ground_truth}\n",
    "Context Chunk: {chunk}\n",
    "\n",
    "Is this chunk relevant for answering the question? Answer only \"RELEVANT\" or \"NOT RELEVANT\".\n",
    "\"\"\")\n",
    "\n",
    "# 3. Run Verification Loop\n",
    "relevance_chain = relevance_prompt | llm | StrOutputParser()\n",
    "\n",
    "verdicts = []\n",
    "print(\"ðŸ” Verifying Chunks...\")\n",
    "\n",
    "for chunk in retrieved_chunks:\n",
    "    result = relevance_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"chunk\": chunk\n",
    "    })\n",
    "    \n",
    "    # Convert text response to binary (1 or 0)\n",
    "    is_relevant = 1 if \"RELEVANT\" in result.upper() else 0\n",
    "    verdicts.append(is_relevant)\n",
    "    \n",
    "    print(f\"Chunk: '{chunk[:30]}...' -> {result.strip()} ({is_relevant})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Precision@K Calculation: Good Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Step 2: The Math (Precision@K Calculation)\n",
      "============================================================\n",
      "Verdicts: [1, 1, 0, 0]\n",
      "\n",
      "Calculation:\n",
      "   Rank 1 (âœ… Relevant): Precision@1 = 1/1 = 1.00 â†’ Adds 1.00\n",
      "   Rank 2 (âœ… Relevant): Precision@2 = 2/2 = 1.00 â†’ Adds 1.00\n",
      "   Rank 3 (âŒ Irrelevant): Precision@3 = 2/3 = 0.67 â†’ Skips\n",
      "   Rank 4 (âŒ Irrelevant): Precision@4 = 2/4 = 0.50 â†’ Skips\n",
      "\n",
      "   Sum of precisions: 2.00\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ðŸ“Š Final Context Precision: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for our Travel Bot results\n",
    "# Verdicts from Step 1: [1 (Rel), 1 (Rel), 0 (Irrel), 0 (Irrel)]\n",
    "verdicts = [1, 1, 0, 0]\n",
    "\n",
    "print(\"ðŸ“Š Step 2: The Math (Precision@K Calculation)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Verdicts: {verdicts}\")\n",
    "print(\"\\nCalculation:\")\n",
    "\n",
    "precisions = []\n",
    "relevant_count = 0\n",
    "\n",
    "for k, is_relevant in enumerate(verdicts, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    \n",
    "    precision_at_k = relevant_count / k\n",
    "    \n",
    "    status = \"âœ… Relevant\" if is_relevant else \"âŒ Irrelevant\"\n",
    "    # We only calculate/add precision if the item at k is relevant\n",
    "    contribution = f\"â†’ Adds {precision_at_k:.2f}\" if is_relevant else \"â†’ Skips\"\n",
    "    \n",
    "    print(f\"   Rank {k} ({status}): Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contribution}\")\n",
    "    \n",
    "    if is_relevant:\n",
    "        precisions.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(verdicts)\n",
    "context_precision = sum(precisions) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of precisions: {sum(precisions):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ðŸ“Š Final Context Precision: {context_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š BAD RANKING: Relevant chunks at BOTTOM\n",
      "============================================================\n",
      "\n",
      "Ranking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 0/1 = 0.00 â†’ Does NOT contribute\n",
      "   Position 2: Precision@2 = 0/2 = 0.00 â†’ Does NOT contribute\n",
      "   Position 3: Precision@3 = 1/3 = 0.33 â†’ Contributes\n",
      "   Position 4: Precision@4 = 2/4 = 0.50 â†’ Contributes\n",
      "\n",
      "   Sum of contributing precisions: 0.83\n",
      "   Total relevant items: 2\n",
      "\n",
      "   ðŸ“Š Context Precision (Bad Ranking): 0.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@K for BAD ranking (relevant at bottom)\n",
    "\n",
    "# Bad ranking: [Not Relevant, Not Relevant, Relevant, Relevant]\n",
    "bad_ranking = [False, False, True, True]\n",
    "\n",
    "print(\"ðŸ“Š BAD RANKING: Relevant chunks at BOTTOM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [âŒ Not Rel, âŒ Not Rel, âœ… Relevant, âœ… Relevant]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_bad = []\n",
    "relevant_count = 0\n",
    "for k, is_relevant in enumerate(bad_ranking, 1):\n",
    "    if is_relevant:\n",
    "        relevant_count += 1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"â†’ Contributes\" if is_relevant else \"â†’ Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {precision_at_k:.2f} {contributes}\")\n",
    "    if is_relevant:\n",
    "        precisions_bad.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(bad_ranking)\n",
    "context_precision_bad = sum(precisions_bad) / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_bad):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   ðŸ“Š Context Precision (Bad Ranking): {context_precision_bad:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Context Recall Deep Dive\n",
    "\n",
    "### What Context Recall Measures\n",
    "\n",
    "**Context Recall** checks if you retrieved *all the necessary information* to answer the question. It measures **retrieval completeness**.\n",
    "\n",
    "\n",
    "### ðŸ”§ How It Works\n",
    "\n",
    "1. Break down the **reference answer** into individual claims\n",
    "2. Check if each claim can be **attributed** to the retrieved context\n",
    "3. Calculate: claims found / total claims\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Context Recall} = \\frac{\\text{Reference claims found in context}}{\\text{Total claims in reference}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Ground Truth Claims \n",
    "\n",
    "First, we identify the specific facts we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Claims to Find: ['Employees may spend up to $75', 'Receipts are required for expenses over $25']\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup data\n",
    "ground_truth = \"Employees may spend up to $75. Receipts are required for expenses over $25.\"\n",
    "retrieved_context = [\n",
    "    \"Employees may spend up to $75 on client dinners.\",\n",
    "    \"All expenses must be reported in Workday.\"\n",
    "]\n",
    "\n",
    "# (In Ragas, an LLM does this extraction. Here is the output:)\n",
    "gt_claims = [\n",
    "    \"Employees may spend up to $75\",           # Claim 1\n",
    "    \"Receipts are required for expenses over $25\" # Claim 2\n",
    "]\n",
    "\n",
    "print(f\"Target Claims to Find: {gt_claims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Attribution Check (The Judge) \n",
    "\n",
    "We use the LLM to check if each Ground Truth claim exists inside the retrieved text. We use a specific prompt pattern to perform this \"Attribution\" check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying Claims in Context...\n",
      "Claim: 'Employees may spend up to $75' -> YES (âœ… Found)\n",
      "Claim: 'Receipts are required for expenses over $25' -> NO (âŒ MISSING)\n"
     ]
    }
   ],
   "source": [
    "# Define Verification Prompt\n",
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Can the following claim be attributed to (found in) the given context?\n",
    "Context: {context}\n",
    "Claim: {claim}\n",
    "\n",
    "Answer \"YES\" if the claim is supported by the context, \"NO\" if it cannot be found.\n",
    "\"\"\")\n",
    "\n",
    "# Run Verification Loop\n",
    "attribution_chain = attribution_prompt | llm | StrOutputParser()\n",
    "attribution_results = []\n",
    "\n",
    "print(\"ðŸ” Verifying Claims in Context...\")\n",
    "\n",
    "# Join context into one block for easier checking\n",
    "full_context_text = \"\\n\".join(retrieved_context)\n",
    "\n",
    "for claim in gt_claims:\n",
    "    result = attribution_chain.invoke({\n",
    "        \"context\": full_context_text,\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    \n",
    "    found = 1 if \"YES\" in result.upper() else 0\n",
    "    attribution_results.append(found)\n",
    "    \n",
    "    status = \"âœ… Found\" if found else \"âŒ MISSING\"\n",
    "    print(f\"Claim: '{claim}' -> {result.strip()} ({status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Math \n",
    "\n",
    "Now we calculate the score based on our findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Recall Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Total Ground Truth Claims: 2\n",
    "# Claims Successfully Found: 1\n",
    "\n",
    "context_recall = 1 / 2\n",
    "\n",
    "print(f\"Context Recall Score: {context_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Context Entity Recall Deep Dive\n",
    "\n",
    "### What Context Entity Recall Measures\n",
    "\n",
    "**Context Entity Recall** checks if you retrieved context containing all the *important entities* (people, places, dates, organizations) mentioned in the reference answer.\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Entity Recall} = \\frac{\\text{Entities in both reference AND context}}{\\text{Total entities in reference}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Entities (The Judge) \n",
    "We ask the LLM to identify the \"Must Have\" entities from the Ground Truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Entities: ```python\n",
      "entities = [\n",
      "    \"Sarah Jenkins\",  # Person\n",
      "    \"VP of Finance\",  # Role\n",
      "    \"$75\"            # Monetary Value\n",
      "]\n",
      "```\n",
      "Retrieved Text Length: 122 chars\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup data\n",
    "ground_truth = \"Expenses over $75 must be approved by Sarah Jenkins, the VP of Finance.\"\n",
    "retrieved_chunks = [\n",
    "    \"Dinner expenses over $75 require approval from the VP of Finance.\",\n",
    "    \"Sarah Jenkins handles all international travel requests.\"\n",
    "]\n",
    "\n",
    "# Compute the full text for comparison\n",
    "retrieved_text = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "# 2. Define Entity Extraction Prompt\n",
    "entity_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract all Named Entities (Person, Organization, Date, Location, Monetary Values, Roles) from the text.\n",
    "Return a Python list of strings.\n",
    "\n",
    "Text: {text}\n",
    "Entities:\n",
    "\"\"\")\n",
    "\n",
    "# 3. Extract Entities from Ground Truth\n",
    "extraction_chain = entity_prompt | llm | StrOutputParser()\n",
    "\n",
    "# This asks the LLM to find the entities in the Ground Truth\n",
    "gt_entities = extraction_chain.invoke({\"text\": ground_truth})\n",
    "\n",
    "# Simulated result from the LLM:\n",
    "print(f\"Ground Truth Entities: {gt_entities}\")\n",
    "print(f\"Retrieved Text Length: {len(retrieved_text)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Comparison Check \n",
    "\n",
    "Now we check if these specific strings appear in our retrieved_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking Entities...\n",
      "   âœ… Found: '$75'\n",
      "   âœ… Found: 'Sarah Jenkins'\n",
      "   âœ… Found: 'VP of Finance'\n",
      "Entities Found List: ['$75', 'Sarah Jenkins', 'VP of Finance']\n"
     ]
    }
   ],
   "source": [
    "# We compare the GT entities against the computed Retrieved Text\n",
    "gt_entities = [\"$75\", \"Sarah Jenkins\", \"VP of Finance\"]\n",
    "entities_found = []\n",
    "context_lower = retrieved_text.lower()\n",
    "\n",
    "print(\"ðŸ” Checking Entities...\")\n",
    "for entity in gt_entities:\n",
    "    if entity.lower() in context_lower:\n",
    "        entities_found.append(entity)\n",
    "        print(f\"   âœ… Found: '{entity}'\")\n",
    "    else:\n",
    "        print(f\"   âŒ MISSING: '{entity}'\")\n",
    "\n",
    "print(f\"Entities Found List: {entities_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Recall Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Total GT Entities: 3\n",
    "# Found in Context: 3\n",
    "\n",
    "entity_recall = 3 / 3\n",
    "\n",
    "print(f\"Entity Recall Score: {entity_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Noise Sensitivity Deep Dive\n",
    "\n",
    "### What Noise Sensitivity Measures\n",
    "\n",
    "**Noise Sensitivity** tests how much *irrelevant information* in the retrieved context causes errors in the answer. It measures **robustness** to noise.\n",
    "\n",
    "\n",
    "### âš ï¸ Important: Lower is Better!\n",
    "\n",
    "Unlike other metrics where higher is better, for Noise Sensitivity:\n",
    "- **0.0** = Great! Model ignores noise completely\n",
    "- **1.0** = Bad! Model is very easily confused by irrelevant information\n",
    "\n",
    "### ðŸ“ Formula\n",
    "\n",
    "$$\\text{Noise Sensitivity} = \\frac{\\text{Incorrect claims from noisy context}}{\\text{Total claims}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Claims from Bot AnswerÂ \n",
    "We break the answer down into atomic facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Claims:  \n",
      "1. The standard room rate is $250.\n",
      "2. The previous room rate was $150.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup data\n",
    "bot_response = \"The standard room rate is $250, but it was previously $150.\"\n",
    "ground_truth = \"The standard room rate is $250.\"\n",
    "\n",
    "# 2. Define Extraction Prompt\n",
    "claim_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract claims from the text. Return a list of strings.\n",
    "Text: {text}\n",
    "Claims:\n",
    "\"\"\")\n",
    "\n",
    "# 3. Run Extraction\n",
    "claim_chain = claim_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Simulated Output from LLM\n",
    "claims = claim_chain.invoke({\"text\": bot_response}) \n",
    "\n",
    "print(f\"Extracted Claims:  \\n{claims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Verification against Ground TruthÂ \n",
    "\n",
    "We verify each claim against the Ground Truth (NOT the retrieved context). This is key. We want to know if the Bot is saying things outside the perfect answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Verifying Claims against Ground Truth...\n",
      "   Claim: 'The standard room rate is $250' -> âœ… Supported\n",
      "   Claim: 'It was previously $150' -> âŒ NOT in GT (Noise Detected)\n"
     ]
    }
   ],
   "source": [
    "claims = [\n",
    "    \"The standard room rate is $250\",\n",
    "    \"It was previously $150\"\n",
    "]\n",
    "\n",
    "# Define Verification Prompt\n",
    "verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Is the following claim supported by the Ground Truth?\n",
    "Ground Truth: {ground_truth}\n",
    "Claim: {claim}\n",
    "\n",
    "Answer \"SUPPORTED\" or \"NOT SUPPORTED\".\n",
    "\"\"\")\n",
    "\n",
    "verification_chain = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "incorrect_claims = 0\n",
    "total_claims = len(claims)\n",
    "\n",
    "print(\"ðŸ” Verifying Claims against Ground Truth...\")\n",
    "\n",
    "for claim in claims:\n",
    "    result = verification_chain.invoke({\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"claim\": claim\n",
    "    })\n",
    "    \n",
    "    if \"NOT SUPPORTED\" in result:\n",
    "        incorrect_claims += 1\n",
    "        print(f\"   Claim: '{claim}' -> âŒ NOT in GT (Noise Detected)\")\n",
    "    else:\n",
    "        print(f\"   Claim: '{claim}' -> âœ… Supported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Sensitivity Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "# Incorrect Claims (Noise): 1\n",
    "# Total Claims: 2\n",
    "\n",
    "noise_sensitivity = 1 / 2\n",
    "\n",
    "print(f\"Noise Sensitivity Score: {noise_sensitivity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Summary & Key Takeaways\n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Metric | What It Measures | Calculation | Ideal |\n",
    "|--------|-----------------|-------------|-------|\n",
    "| **Faithfulness** | Hallucination detection | Supported claims / Total claims | 1.0 |\n",
    "| **Answer Relevancy** | Answer addresses question | Avg cosine similarity of generated Qs | 1.0 |\n",
    "| **Context Precision** | Ranking quality | Position-weighted precision | 1.0 |\n",
    "| **Context Recall** | Retrieval completeness | Reference claims found / Total | 1.0 |\n",
    "| **Entity Recall** | Entity coverage | Common entities / Reference entities | 1.0 |\n",
    "| **Noise Sensitivity** | Robustness to noise | Incorrect claims / Total claims | 0.0 |\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "1. **Faithfulness** and **Answer Relevancy** evaluate the **Generator (LLM)**\n",
    "2. **Context Precision**, **Context Recall**, and **Entity Recall** evaluate the **Retriever**\n",
    "3. **Noise Sensitivity** evaluates the **overall system's robustness**\n",
    "4. Understanding the **intermediate steps** helps debug evaluation issues\n",
    "5. Use these metrics **together** for comprehensive RAG evaluation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
